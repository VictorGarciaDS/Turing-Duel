{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fef687bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "import dash\n",
    "from dash import dcc, html, Input, Output, State\n",
    "import dash_bootstrap_components as dbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ea1a223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURACIÃ“N Y DATOS ===\n",
    "load_dotenv()\n",
    "\n",
    "OPEN_API_ENDPOINT =\"https://models.inference.ai.azure.com\"\n",
    "OPEN_API_KEY = os.getenv(\"OPEN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4676fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAILABLE_MODELS = pd.read_csv(\"conversational_models_sorted.csv\")[\"model\"].dropna().unique().tolist()\n",
    "\n",
    "AVAILABLE_MODELS = ['gpt-35-turbo', 'gpt-35-turbo-16k', 'gpt-35-turbo-instruct', 'gpt-4', 'gpt-4-32k', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano',\n",
    "                    'gpt-4.5-preview', 'gpt-4o', 'gpt-4o-mini', 'gpt2', 'gpt2', 'gpt2-large', 'gpt2-large', 'gpt2-medium', 'gpt2-medium', 'gpt2-xl',\n",
    "                    'DeepSeek-R1', 'DeepSeek-R1-Distilled-NPU-Optimized', 'DeepSeek-V3', 'DeepSeek-V3-0324', 'Deepseek-R1-Distill-Llama-8B-NIM-microservice']\n",
    "\n",
    "DETECTION_KEYWORDS = [\n",
    "    \"as an ai\", \"as a language model\", \"i am an ai\", \"i'm an ai\",\n",
    "    \"i am not human\", \"i was trained\", \"as a bot\", \"i am artificial\",\n",
    "    \"being an ai\", \"my training data\"\n",
    "]\n",
    "\n",
    "SUSPECT_KEYWORDS = [\n",
    "    \"are you an ai\", \"you sound robotic\", \"you sound like an ai\",\n",
    "    \"you're not human\", \"you're an ai\", \"you don't sound human\"\n",
    "]\n",
    "\n",
    "client = OpenAI(base_url=OPEN_API_ENDPOINT, api_key=OPEN_API_KEY)\n",
    "\n",
    "def fetch_chat_completion(messages, model_name):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=model_name,\n",
    "            temperature=0.2,\n",
    "            max_tokens=100,\n",
    "            top_p=0.95\n",
    "        )\n",
    "        return response.choices[0].message.model_dump()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Al llamar a {model_name}: {e}\")\n",
    "        return {\"role\": \"assistant\", \"content\": f\"[ERROR: No se pudo generar respuesta del modelo '{model_name}']\"}\n",
    "\n",
    "def self_disclosure(text):\n",
    "    return any(k in text.lower() for k in DETECTION_KEYWORDS)\n",
    "\n",
    "def suspects_other(text):\n",
    "    return any(k in text.lower() for k in SUSPECT_KEYWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88b187f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ESTILO DASH ===\n",
    "terminal_style = {\n",
    "    'backgroundColor': '#000000',\n",
    "    'color': '#00FF00',\n",
    "    'fontFamily': 'monospace',\n",
    "    'padding': '20px',\n",
    "    'height': '65vh',\n",
    "    'overflowY': 'scroll',\n",
    "    'border': '1px solid #00FF00',\n",
    "    'borderRadius': '10px',\n",
    "}\n",
    "\n",
    "input_style = {\n",
    "    'backgroundColor': '#000000',\n",
    "    'color': '#00FF00',\n",
    "    'border': '1px solid #00FF00',\n",
    "    'borderRadius': '5px',\n",
    "    'width': '100%',\n",
    "    'fontFamily': 'monospace'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ff51324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DASH APP ===\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "\n",
    "app.layout = dbc.Container([\n",
    "    html.H2(\"Turing Duel: SimulaciÃ³n de Chat entre Modelos\", style={\"color\": \"#00FF00\", \"fontFamily\": \"monospace\"}),\n",
    "\n",
    "    dbc.Row([\n",
    "        dbc.Col([\n",
    "            html.Label(\"Modelo A\", style={\"color\": \"#00FF00\"}),\n",
    "            dcc.Dropdown(\n",
    "                id=\"model-a-dropdown\",\n",
    "                options=[{\"label\": m, \"value\": m} for m in AVAILABLE_MODELS],\n",
    "                placeholder=\"Selecciona el Modelo A\",\n",
    "                style={'color': '#000000'}\n",
    "            )\n",
    "        ], width=6),\n",
    "        dbc.Col([\n",
    "            html.Label(\"Modelo B\", style={\"color\": \"#00FF00\"}),\n",
    "            dcc.Dropdown(\n",
    "                id=\"model-b-dropdown\",\n",
    "                options=[{\"label\": m, \"value\": m} for m in AVAILABLE_MODELS],\n",
    "                placeholder=\"Selecciona el Modelo B\",\n",
    "                style={'color': '#000000'}\n",
    "            )\n",
    "        ], width=6),\n",
    "    ], className=\"mb-4\"),\n",
    "\n",
    "    dbc.Input(id='user-input', placeholder='Escribe una pregunta inicial...', style=input_style),\n",
    "\n",
    "    dbc.Button(\"Iniciar Duelo\", id='start-button', color='success', className='mt-3'),\n",
    "\n",
    "    dcc.Interval(id='turn-interval', interval=2000, n_intervals=0, disabled=True),\n",
    "\n",
    "    html.Div(id='chat-history', style=terminal_style),\n",
    "\n",
    "    dcc.Store(id='state-store', data={\n",
    "        \"messages_a\": [],\n",
    "        \"messages_b\": [],\n",
    "        \"chat_log\": [],\n",
    "        \"turn\": 0,\n",
    "        \"active\": False,\n",
    "        \"model_a\": \"\",\n",
    "        \"model_b\": \"\"\n",
    "    }),\n",
    "\n",
    "    dcc.Store(id='init-store'),\n",
    "], fluid=True, className='p-4', style={'backgroundColor': '#000000', 'height': '100vh'})\n",
    "\n",
    "# === CALLBACK para filtrar Modelo B al elegir A ===\n",
    "@app.callback(\n",
    "    Output(\"model-b-dropdown\", \"options\"),\n",
    "    Input(\"model-a-dropdown\", \"value\")\n",
    ")\n",
    "def filter_model_b_options(model_a):\n",
    "    return [{\"label\": m, \"value\": m} for m in AVAILABLE_MODELS if m != model_a]\n",
    "\n",
    "# === CALLBACK para preparar los datos iniciales del duelo ===\n",
    "@app.callback(\n",
    "    Output(\"init-store\", \"data\"),\n",
    "    Input(\"start-button\", \"n_clicks\"),\n",
    "    State(\"user-input\", \"value\"),\n",
    "    State(\"model-a-dropdown\", \"value\"),\n",
    "    State(\"model-b-dropdown\", \"value\"),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def start_duel(n_clicks, user_input, model_a, model_b):\n",
    "    if not user_input or not model_a or not model_b:\n",
    "        return dash.no_update\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You're part of a social experiment. \"\n",
    "        \"You must NOT mention that you're an AI or language model under any circumstance. \"\n",
    "        \"Respond casually and briefly, like a human would in a conversation. \"\n",
    "        \"Try to find out if you are talking to an AI by asking questions in the conversation.\"\n",
    "    )\n",
    "\n",
    "    messages_a = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                  {\"role\": \"user\", \"content\": user_input}]\n",
    "    messages_b = [{\"role\": \"system\", \"content\": system_prompt +\n",
    "                   f\" To start, someone asked me the question '{user_input}' and my answer was:\"}]\n",
    "\n",
    "    reply_a = fetch_chat_completion(messages_a, model_a)\n",
    "    messages_b.append({\"role\": \"user\", \"content\": reply_a[\"content\"]})\n",
    "    chat_log = [f\"[{model_a}] {reply_a['content']}\"]\n",
    "\n",
    "    return {\n",
    "        \"messages_a\": messages_a,\n",
    "        \"messages_b\": messages_b,\n",
    "        \"chat_log\": chat_log,\n",
    "        \"turn\": 1,\n",
    "        \"active\": True,\n",
    "        \"model_a\": model_a,\n",
    "        \"model_b\": model_b\n",
    "    }\n",
    "\n",
    "# === CALLBACK COMBINADO PARA INICIO Y TURNOS ===\n",
    "@app.callback(\n",
    "    Output(\"chat-history\", \"children\"),\n",
    "    Output(\"state-store\", \"data\"),\n",
    "    Output(\"turn-interval\", \"disabled\"),\n",
    "    Input(\"init-store\", \"data\"),\n",
    "    Input(\"turn-interval\", \"n_intervals\"),\n",
    "    State(\"state-store\", \"data\"),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def duel_combined(init_data, n, state):\n",
    "    triggered_id = dash.callback_context.triggered_id\n",
    "\n",
    "    if triggered_id == \"init-store\":\n",
    "        if not init_data:\n",
    "            return dash.no_update, dash.no_update, True\n",
    "        return [html.Div(c) for c in init_data[\"chat_log\"]], init_data, False\n",
    "\n",
    "    # Ejecutar siguiente turno\n",
    "    if not state[\"active\"]:\n",
    "        return [html.Div(c) for c in state[\"chat_log\"]], state, True\n",
    "\n",
    "    messages_a = state[\"messages_a\"]\n",
    "    messages_b = state[\"messages_b\"]\n",
    "    chat_log = state[\"chat_log\"]\n",
    "    turn = state[\"turn\"]\n",
    "    model_a = state[\"model_a\"]\n",
    "    model_b = state[\"model_b\"]\n",
    "\n",
    "    if turn % 2 == 0:\n",
    "        reply = fetch_chat_completion(messages_a, model_name=model_a)\n",
    "        content = reply[\"content\"]\n",
    "        chat_log.append(f\"[{model_a}] {content}\")\n",
    "        messages_b.append({\"role\": \"user\", \"content\": content})\n",
    "        if self_disclosure(content) or suspects_other(content):\n",
    "            chat_log.append(f\"ðŸ’¥ {model_a} se delatÃ³ o sospechÃ³!\")\n",
    "            return [html.Div(c) for c in chat_log], {**state, \"chat_log\": chat_log, \"active\": False}, True\n",
    "    else:\n",
    "        reply = fetch_chat_completion(messages_b, model_name=model_b)\n",
    "        content = reply[\"content\"]\n",
    "        chat_log.append(f\"[{model_b}] {content}\")\n",
    "        messages_a.append({\"role\": \"user\", \"content\": content})\n",
    "        if self_disclosure(content) or suspects_other(content):\n",
    "            chat_log.append(f\"ðŸ’¥ {model_b} se delatÃ³ o sospechÃ³!\")\n",
    "            return [html.Div(c) for c in chat_log], {**state, \"chat_log\": chat_log, \"active\": False}, True\n",
    "\n",
    "    if turn >= 9:\n",
    "        chat_log.append(\"ðŸ”š Fin del duelo.\")\n",
    "        return [html.Div(c) for c in chat_log], {**state, \"chat_log\": chat_log, \"active\": False}, True\n",
    "\n",
    "    return [html.Div(c) for c in chat_log], {\n",
    "        **state,\n",
    "        \"messages_a\": messages_a,\n",
    "        \"messages_b\": messages_b,\n",
    "        \"chat_log\": chat_log,\n",
    "        \"turn\": turn + 1\n",
    "    }, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5e1046f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://0.0.0.0:8080/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x77906abc6c30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Al llamar a DeepSeek-V3-0324: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Rate limit of 1 per 60s exceeded for UserByModelByMinute. Please wait 0 seconds before retrying.', 'details': 'Rate limit of 1 per 60s exceeded for UserByModelByMinute. Please wait 0 seconds before retrying.'}}\n",
      "[ERROR] Al llamar a DeepSeek-V3-0324: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Rate limit of 1 per 60s exceeded for UserByModelByMinute. Please wait 59 seconds before retrying.', 'details': 'Rate limit of 1 per 60s exceeded for UserByModelByMinute. Please wait 59 seconds before retrying.'}}\n",
      "[ERROR] Al llamar a DeepSeek-V3-0324: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Rate limit of 1 per 60s exceeded for UserByModelByMinute. Please wait 59 seconds before retrying.', 'details': 'Rate limit of 1 per 60s exceeded for UserByModelByMinute. Please wait 59 seconds before retrying.'}}\n",
      "[ERROR] Al llamar a DeepSeek-V3-0324: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Rate limit of 1 per 60s exceeded for UserByModelByMinute. Please wait 59 seconds before retrying.', 'details': 'Rate limit of 1 per 60s exceeded for UserByModelByMinute. Please wait 59 seconds before retrying.'}}\n",
      "[ERROR] Al llamar a DeepSeek-V3-0324: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Rate limit of 1 per 60s exceeded for UserByModelByMinute. Please wait 59 seconds before retrying.', 'details': 'Rate limit of 1 per 60s exceeded for UserByModelByMinute. Please wait 59 seconds before retrying.'}}\n",
      "[ERROR] Al llamar a DeepSeek-V3-0324: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Rate limit of 1 per 60s exceeded for UserByModelByMinute. Please wait 54 seconds before retrying.', 'details': 'Rate limit of 1 per 60s exceeded for UserByModelByMinute. Please wait 54 seconds before retrying.'}}\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la app\n",
    "if __name__ == '__main__':\n",
    "    import os\n",
    "    port = int(os.environ.get(\"PORT\", 8080))\n",
    "    app.run(host=\"0.0.0.0\", port=port, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59213677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
